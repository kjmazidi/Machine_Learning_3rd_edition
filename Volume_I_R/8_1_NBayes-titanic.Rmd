---
title: "Naive Bayes on the Titanic Data"
author: "Karen Mazidi"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---

Performing Naive Bayes on the Titanic data set.

### Load and preprocess the data

We will skip the description of these steps since they are the same as in the logistic regression example.

```{r}
df <- read.csv("data/titanic.csv", header=TRUE)

# data cleaning
df <- df[,c(1,2,4,5)]
df$pclass <- factor(df$pclass)
df$survived <- factor(df$survived)

# handle missing values
df$age[is.na(df$age)] <- median(df$age,na.rm=T)
```


### Divide into train and test sets 

This should be the same split as we had for logistic regression so we can compare the two algorithms.


```{r}
set.seed(1234)
i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
train <- df[i,]
test <- df[-i,]

```
### Build the model

The naive Bayes algorithm is in package e1071.

```{r}
library(e1071)
nb1 <- naiveBayes(survived~., data=train)
nb1
```

### Evaluate on the test data

```{r}
p1 <- predict(nb1, newdata=test, type="class")
table(p1, test$survived)
mean(p1==test$survived)
```

We got very slightly higher for naive Bayes than we did for logistic regression. 

### Extracting probabilities

One of the nice things about the algorithm is that you can extract the raw probabilities.

```{r}
p1_raw <- predict(nb1, newdata=test, type="raw")
head(p1_raw)
```

#### Remove Age

When we look at the Naive Bayes algorithm, we see the mean for survived versus perished is different by only one year. This suggests that age has little predictive value. Let's check that by building another model, this time without age.

```{r}
nb2 <- naiveBayes(survived~.-age, data=train)
p2 <- predict(nb2, newdata=test[,-4], type="class")
table(p2, test$survived)
mean(p2==test$survived)
```

As it turns out, there is only a very slight difference in the accuracies.