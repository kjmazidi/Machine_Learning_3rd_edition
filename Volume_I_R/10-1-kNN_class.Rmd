---
title: "10-1-kNN_class"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: united
---

### Code Accompanying ***The Machine Learning Handbooks***, Volume I, Chapter 10

#### Book pdf is available on the GitHub repo: https://github.com/kjmazidi/Machine_Learning_3rd_edition

###### Copyright KJG Mazidi

### Introduction

This notebook looks at the built-in data set **iris** as an introductory example of kNN classification in R.

Library imported: class for kNN classification


```{r, warning = FALSE, message = FALSE}
if (!require(class)){
  install.packages("class")
}
library("class")
```



### Load and look at the data 

The iris data set has 150 instances and 5 columns:
- Sepal.Length
- Sepal.Width
- Petal.Length
- Petal.Width
- Species: setosa, versicolor or virginica

```{r}
str(iris)    # display the structure of the object
summary(iris)
```

### Plot the data

We let the 3 classes show as 3 different colors with the bg parameter and the "unclass" values 1, 2, 3 representing the 3 types of irises.

```{r}
plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")
     [unclass(iris$Species)], main="Iris Data")
```

### Pairs scatter plots

```{r}
pairs(iris[1:4], main = "Iris Data", pch = 21, bg = c("red", "green3", "blue")[unclass(iris$Species)])
```

### Divide into train/test sets

We will randomly sample the data set to let 2/3 be training and 1/3 test, 

```{r}
set.seed(1958)  # setting a seed gets the same results every time
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))
iris.train <- iris[ind==1, 1:4]
iris.test <- iris[ind==2, 1:4]
iris.trainLabels <- iris[ind==1, 5]
iris.testLabels <- iris[ind==2, 5]
```

### Classify

The knn() function uses Euclidean distance to find the k nearest neighbors.

Classificiation is decided by majority vote with ties broken at random. 

Using an odd k can avoid some ties.

```{r}
iris_pred <- knn(train=iris.train, test=iris.test, cl=iris.trainLabels, k=3)
```

### Compute accuracy

We built a classifier with very high accuracy. Note that the iris data set is quite easy to classify. 

It's often a good idea to scale the variables for clustring to make the distance calculations better. However in this case, the 3 predictors are in the same scale so it's not necessary.

```{r}
results <- iris_pred == iris.testLabels
acc <- length(which(results==TRUE)) / length(results)
# or combine into one line:
#acc <- length(which(iris_pred == iris.testLabels)) / length(iris_pred)
table(results, iris_pred)
acc
```

