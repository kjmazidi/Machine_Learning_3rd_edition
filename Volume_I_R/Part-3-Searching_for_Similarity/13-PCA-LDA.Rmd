---
title: "PCA and LDA"
author: "Karen Mazidi"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

### Run PCA on the iris data

```{r}
library(caret)
data(iris)
i <- sample(1:150, 100, replace=FALSE)
train <- iris[i,]
test <- iris[-i,]
set.seed(1234)
pca_out <- preProcess(train[,1:4], method=c("center", "scale", "pca"))
pca_out

```
### PCA plot

```{r}
train_pc <- predict(pca_out, train[, 1:4])
test_pc <- predict(pca_out, test[,])
plot(test_pc$PC1, test_pc$PC2, pch=c(23,21,22)[unclass(test_pc$Species)], bg=c("red","green","blue")[unclass(test$Species)])
```
### PCA data in knn

Now let's see if our two principal components can predict class.


```{r}
train_df <- data.frame(train_pc$PC1, train_pc$PC2, train$Species)
test_df <- data.frame(test_pc$PC1, test_pc$PC2, test$Species)
library(class)
set.seed(1234)
pred <- knn(train=train_df[,1:2], test=test_df[,1:2], cl=train_df[,3], k=3)
mean(pred==test$Species)
```

The accuracy is lower than if we used all 4 predictors. 

```{r}
library(tree)
colnames(train_df) <- c("PC1", "PC2", "Species")
colnames(test_df) <- c("PC1", "PC2", "Species")
set.seed(1234)
tree1 <- tree(Species~., data=train_df)
plot(tree1)
text(tree1, cex=0.5, pretty=0)

pred <- predict(tree1, newdata=test_df, type="class")
mean(pred==test$Species)
```

With the decison tree we got a little lower accuracy.

### LDA

```{r}
library(MASS)
lda1 <- lda(Species~., data=train)
lda1$means
```

### predict on test

```{r}
lda_pred <- predict(lda1, newdata=test, type="class")
lda_pred$class
mean(lda_pred$class==test$Species)
```

### plot

```{r}
plot(lda_pred$x[,1], lda_pred$x[,2], pch=c(23,21,22)[unclass(lda_pred$class)], bg=c("red","green","blue")[unclass(test_pc$Species)])
```

