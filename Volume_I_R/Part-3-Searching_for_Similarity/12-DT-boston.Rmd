---
title: "12-DT-boston"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: united
---

### Code Accompanying ***The Machine Learning Handbooks***, Volume I, Chapter 12

#### Book pdf is available on the GitHub repo: https://github.com/kjmazidi/Machine_Learning_3rd_edition

###### Copyright KJG Mazidi

### Introduction

This notebook explores decision tree regression. 

Library imports: 

* tree
* MASS for the Boston data set
* randomForest


```{r, warning = FALSE, message = FALSE}

if (!require(tree)){
  install.packages("tree")
}
library("tree")

if (!require(MASS)){
  install.packages("MASS")
}
library("MASS")

if (!require(randomForest)){
  install.packages("randomForest")
}
library("randomForest")

```


### Try linear regression on Boston

First we try linear regression as a baseline. Look at the correlation (corr_lm) and rmse_lm in the environment pane. 


```{r}
names(Boston)

# divide into train and test
set.seed(1234)
i <- sample(nrow(Boston), 0.8*nrow(Boston), replace = FALSE)
train <- Boston[i,]
test <- Boston[-i,]
lm1 <- lm(medv~., data=train)
summary(lm1)
pred <- predict(lm1, newdata=test)
cor_lm <- cor(pred, test$medv)
rmse_lm <- sqrt(mean((pred-test$medv)^2))
```

### Using tree

Now compare the results (see Environment pane) with a decision tree. 

```{r}
tree1 <- tree(medv~., data=train)
summary(tree1)
pred <- predict(tree1, newdata=test)
print(paste('correlation:', cor(pred, test$medv)))
rmse_tree <- sqrt(mean((pred-test$medv)^2))
print(paste('rmse:', rmse_tree))
plot(tree1)
text(tree1, cex=0.5, pretty=0)
```


### cross validation

See the chapter for further description. 

```{r}
cv_tree <- cv.tree(tree1)
plot(cv_tree$size, cv_tree$dev, type='b')
```

### prune the tree

See the chapter for further description. 


```{r}
tree_pruned <- prune.tree(tree1, best=5)
plot(tree_pruned)
text(tree_pruned, pretty=0)
```


### test on the pruned tree

See the chapter for further description. 

In this case pruning did not improve results on the test data but the tree is simpler and easier to interpret.


```{r}
pred_pruned <- predict(tree_pruned, newdata=test)
cor_pruned <- cor(pred_pruned, test$medv)
rmse_pruned <- rmse_pruned <- sqrt(mean((pred_pruned-test$medv)^2))
```

### Random Forest

The importance=TRUE argument tells the algorithm to consider the importance of predictors. 

```{r}
# uses library(randomForest)
set.seed(1234)
rf <- randomForest(medv~., data=train, importance=TRUE)
rf
```


### predict on the random forest

See the chapter for further description. 

```{r}
pred_rf <- predict(rf, newdata=test)
cor_rf <- cor(pred_rf, test$medv)
print(paste('corr:', cor_rf))
rmse_rf <- sqrt(mean((pred_rf-test$medv)^2))
print(paste('rmse:', rmse_rf))
```


### bagging

Setting mtry to the number of predictors, p, will result in bagging

```{r}
bag <- randomForest(medv~., data=train, mtry=13)
bag
```

### predict

Our results for bagging were slightly lower than for the random forest.

```{r}
pred_bag <- predict(bag, newdata=test)
cor_bag <- cor(pred_bag, test$medv)
rmse_bag <- sqrt(mean((pred_bag-test$medv)^2))
```

