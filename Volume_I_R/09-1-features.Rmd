---
title: "09-1-features"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: united
---

### Code Accompanying ***The Machine Learning Handbooks***, Volume I, Chapter 9

#### Book pdf is available on the GitHub repo: https://github.com/kjmazidi/Machine_Learning_3rd_edition

###### Copyright KJG Mazidi

### Introduction

This notebook demonstrates feature selection to identify the most predictive features. 

Packages imported in this notebook:

- caret for feature comparison
- FSelector for feature selection
- mlbench for the PimaIndiansDiabetes2 data

```{r, warning = FALSE, message = FALSE}
if (!require(caret)){
  install.packages("caret")
}
library("caret")

if (!require(mlbench)){
  install.packages("mlbench")
}
library("mlbench")

# Notebook will be revised when FSelector decides to play nicely with Apple M chips
#if (!require(FSelector)){
#  install.packages("FSelector")
#}
#library("FSelector")
```

### Look for correlations in Pima data

The findCorrelation() function suggests that we could remove column 6, mass, because it correlates with triceps. And that we could remove column 2, glucose, because it correlates with insulin. 

```{r}
library(caret)
library(mlbench)
data("PimaIndiansDiabetes2")
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
corMatrix <- cor(df[,1:7])
findCorrelation(corMatrix, cutoff=0.5, verbose=TRUE)
```

### Remove the highly correlated columns

```{r}
df <- df[,-c(2,6)]
```


### Rank features

The varImp() function ranks variables by importance. It requires a model which we trained on method knn, using control parameters stored in variable ctrl.

```{r}
ctrl <- trainControl(method="repeatedcv", repeats=5)
model <- train(diabetes~., data=df, method="knn", preProcess="scale", trControl=ctrl)
importance <- varImp(model, scale=FALSE)
importance
plot(importance)
```
### Recursive feature selection

We start with the data set including all columns.

```{r}
df <- PimaIndiansDiabetes2[complete.cases(PimaIndiansDiabetes2[]),]
ctrl <- rfeControl(functions=rfFuncs, method="cv", number=10)
rfe_out <- rfe(df[,1:7], df[,8], sizes=c(1:7), rfeControl=ctrl)
rfe_out
```

### FSelector

At the time of this notebook update, FSelector was not playing nice with Apple M chips. The notebook will be revised at a later date. 

```{r}
#library(FSelector)
#var_scores <- random.forest.importance(diabetes~., df)
#var_scores
```

